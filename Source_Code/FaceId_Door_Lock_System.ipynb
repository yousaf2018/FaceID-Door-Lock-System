{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fitted-christmas",
   "metadata": {},
   "source": [
    "## Dataset generation using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "working-marker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras\n",
    "import keras_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "from random import shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "print(device)\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compound-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 248, 248, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 248, 248, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 123, 123, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 123, 123, 64)      51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 123, 123, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 61, 61, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 61, 61, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 61, 61, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1048)              1208344   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 4196      \n",
      "=================================================================\n",
      "Total params: 1,726,428\n",
      "Trainable params: 1,725,788\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#AlexNet Architecture for EC estimation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), activation='relu', input_shape=(250,250,1)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1048, activation='relu'),\n",
    "    keras.layers.Dropout(0.8),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generation():\n",
    "    FaceClassifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    def face_detection(image):\n",
    "        gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        detected_face = FaceClassifier.detectMultiScale(gray_scale, 1.3, 5)\n",
    "        \n",
    "        if detected_face is ():\n",
    "            return None\n",
    "        for (x,y,w,h) in detected_face:\n",
    "            face = image[y:y+h,x:x+w]\n",
    "        return face\n",
    "    \n",
    "    videoCapture = cv2.VideoCapture(0)\n",
    "    image_id = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = videoCapture.read()\n",
    "        if face_detection(frame) is not None:\n",
    "            image_id+=1\n",
    "            face = cv2.resize(face_detection(frame), (250,250))\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            #Saving images from web cam to local disk\n",
    "            file_name_path = \"Dataset/\"+\"Yousaf_\"+str(image_id)+'.jpg'\n",
    "            cv2.imwrite(file_name_path, face)\n",
    "            cv2.putText(face, str(image_id), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2 )\n",
    "            cv2.imshow(\"Cropped_Face\", face)\n",
    "            if cv2.waitKey(1)==13 or int(image_id)==1000:\n",
    "                break\n",
    "                \n",
    "    videoCapture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Data Collection is completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bizarre-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "different-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating labels for each person\n",
    "def label(image):\n",
    "    name = image.split(\"_\")[0]\n",
    "    #One hot encoding for four persons \n",
    "    if name == \"Yousaf\":\n",
    "        return 0\n",
    "    elif name == \"Qazi\":\n",
    "        return 1\n",
    "    elif name == \"Abdullah\":\n",
    "        return 2\n",
    "    elif name == \"Manzoor\":\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-battle",
   "metadata": {},
   "source": [
    "## Preparing dataset for deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-virgin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "confident-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataset = []\n",
    "    for image_name in tqdm(os.listdir(\"C:\\\\Users\\\\Marhaba\\\\Videos\\\\FaceID-Door-Lock-System\\\\Source_Code\\\\Dataset\")):\n",
    "        image_path = os.path.join(\"C:\\\\Users\\\\Marhaba\\\\Videos\\\\FaceID-Door-Lock-System\\\\Source_Code\\\\Dataset\",image_name)\n",
    "        image = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)/256\n",
    "        dataset.append([image,label(image_name)])\n",
    "    shuffle(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "similar-tension",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:03<00:00, 812.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fixed-hudson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.15625   , 0.15234375, 0.1484375 , ..., 0.51953125, 0.51953125,\n",
       "         0.515625  ],\n",
       "        [0.15234375, 0.1484375 , 0.1484375 , ..., 0.5234375 , 0.51953125,\n",
       "         0.51953125],\n",
       "        [0.15234375, 0.1484375 , 0.1484375 , ..., 0.52734375, 0.5234375 ,\n",
       "         0.5234375 ],\n",
       "        ...,\n",
       "        [0.21484375, 0.22265625, 0.23046875, ..., 0.53125   , 0.53125   ,\n",
       "         0.53125   ],\n",
       "        [0.22265625, 0.23046875, 0.23828125, ..., 0.53125   , 0.52734375,\n",
       "         0.52734375],\n",
       "        [0.23046875, 0.23828125, 0.24609375, ..., 0.53125   , 0.52734375,\n",
       "         0.52734375]]),\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "amazing-respondent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 250, 250, 1)\n",
      "(900, 250, 250, 1)\n"
     ]
    }
   ],
   "source": [
    "#Splitting dataset into training and testing \n",
    "train_dataset = dataset[:1600]\n",
    "test_dataset = dataset[1600::]\n",
    "X = np.array([k[0] for k in train_dataset]).reshape(-1,250,250,1)\n",
    "print(X.shape)\n",
    "Y = [j[1] for j in train_dataset]\n",
    "Y = to_categorical(Y)\n",
    "X_test = np.array([k[0] for k in test_dataset]).reshape(-1,250,250,1)\n",
    "print(X_test.shape)\n",
    "Y_test = [j[1] for j in test_dataset]\n",
    "Y_test = to_categorical(Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-instrumentation",
   "metadata": {},
   "source": [
    "## CNN architecture for face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "corresponding-reporter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-philosophy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-mitchell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_1:0' shape=() dtype=int32> fp\n",
      "tracking <tf.Variable 'Variable_2:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_3:0' shape=() dtype=int32> fn\n",
      "tracking <tf.Variable 'Variable_4:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_5:0' shape=() dtype=int32> fp\n",
      "tracking <tf.Variable 'Variable_6:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_7:0' shape=() dtype=int32> fn\n",
      "WARNING:tensorflow:From C:\\Users\\Marhaba\\Anaconda3\\envs\\New_Environment_For_Keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1120 samples, validate on 480 samples\n",
      "Epoch 1/17\n",
      " 288/1120 [======>.......................] - ETA: 7:49 - loss: 1.8128 - accuracy: 0.6875 - precision: 0.7668 - recall: 0.7592 - f1_score: 0.7593"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',keras_metrics.precision(),keras_metrics.recall(),keras_metrics.f1_score()])\n",
    "history=model.fit(X, Y, epochs=17, batch_size=32, verbose=1, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,Y_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['precision'])\n",
    "plt.plot(history.history['recall'])\n",
    "plt.plot(history.history['f1_score'])\n",
    "plt.title('Precision Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Number of epocs')\n",
    "plt.legend(['Precision', 'Recall','F1 Score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss','val_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading the model weights\n",
    " \n",
    "# save model\n",
    "model.save_weights('faceRecognitionModel')\n",
    "print('Model Saved!')\n",
    " \n",
    "# load model\n",
    "savedModel = model.load_weights('faceRecognitionModel')\n",
    "print('Model Loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-convention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AlexNet Architecture for EC estimation\n",
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(250,250,1)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1048, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model2.summary()\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',keras_metrics.precision(),keras_metrics.recall(),keras_metrics.f1_score()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-alliance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "savedModel = model2.load_weights('faceRecognitionModel')\n",
    "print('Model Loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(X_test,Y_test,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_model():\n",
    "    FaceClassifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    def face_detection(image):\n",
    "        gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        detected_face = FaceClassifier.detectMultiScale(gray_scale, 1.3, 5)\n",
    "        \n",
    "        if detected_face is ():\n",
    "            return None\n",
    "        for (x,y,w,h) in detected_face:\n",
    "            face = image[y:y+h,x:x+w]\n",
    "        return face\n",
    "    \n",
    "    videoCapture = cv2.VideoCapture(0)    \n",
    "    while True:\n",
    "        ret, frame = videoCapture.read()\n",
    "        if face_detection(frame) is not None:\n",
    "            face = cv2.resize(face_detection(frame), (250,250))\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            cv2.putText(face, str(\"Recognising\"), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2 )\n",
    "            cv2.imshow(\"Cropped_Face\", face)\n",
    "            prediction = model2.predict(face.reshape(-1, 250, 250, 1))\n",
    "            print(prediction)\n",
    "            if cv2.waitKey(1)==13:\n",
    "                break\n",
    "                \n",
    "    videoCapture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Data Collection is completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model2.predict_classes(X_test[90].reshape(-1, 250, 250, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)\n",
    "print(Y_test[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-berlin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-blind",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
